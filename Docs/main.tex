\newcommand{\myname}{\textbf{A. del Val}}

\documentclass[12pt]{article}
\usepackage{sectsty}
%%%% Remove if not good %%%

\usepackage{geometry}
\geometry{a4paper, margin=1.1in, top=0.8in}

%%%%%%%
\usepackage{longtable}
\usepackage[most]{tcolorbox}
\definecolor{block-gray}{gray}{0.85}
\newtcolorbox{myquote}{colback=block-gray,boxrule=0pt,boxsep=0pt,breakable}
% \newtcolorbox{myquote}{colback=block-gray,grow to right by=-10mm,grow to left by=-10mm,
% boxrule=0pt,boxsep=0pt,breakable}
\makeatletter
\def\quoteparse{\@ifnextchar`{\quotex}{\singlequote}}
\def\quotex#1{\@ifnextchar`{\triplequote\@gobble}{\doublequote}}
\makeatother
\def\singlequote#1`{[StartQ]#1[EndQ]\quoteON}
\def\doublequote#1``{[StartQQ]#1[EndQQ]\quoteON}
\long\def\triplequote#1```{\begin{myquote}\parskip 1ex#1\end{myquote}\quoteON}
\def\quoteON{\catcode``=\active}
\def\quoteOFF{\catcode``=12}
\quoteON
\def`{\quoteOFF \quoteparse}
\quoteOFF

\pagenumbering{gobble}

\title{\large \textbf{SoFIA: So}bol-based sensitivity analysis, \textbf{F}orward and \textbf{I}nverse uncertainty propagation with \textbf{A}pplications to high temperature gases}
\date{}
\sectionfont{\fontsize{12}{15}\selectfont}
\begin{document}
\maketitle
% \thispagestyle{fancy}
\vspace{-1.8cm}
\begin{center}
\textbf{Anabel del Val}\\
\vspace{0.5cm}
25/06/2021
\end{center}

\section{Purpose and philosophy of this library}

This library was born out of my PhD studies. The code and tools initially available here are the ones I developed for my research. This library is not meant to be an exhaustive library such as \textit{scikit-learn} but a platform where I deposit my tools after cleaning them and making them more user-friendly. If you go online and type something like: UQ python, you'll find a myriad libraries so why developing my own? I also asked myself this question for a long time, and whenever possible, I used libraries that were already available. Libraries that are developed by a large group of individuals are generally better thought out, more efficient, more robust to changes, and they probably already have all you need. SoFIA is not a library for people wanting to do UQ in general. Other libraries are better. SoFIA is meant to fill-in the gap between the aerothermodynamics community and the UQ community.

Aerothermodynamicists in general have many other things to worry about in their research. We deal with complicated models and experimental facilities, and it is not often the case that we have the required mathematical background to do UQ. Therefore, looking for general purpose UQ libraries can be quite scary and burdensome. Our models are complex and so is our data. If you know me, you know that my PhD research was oriented towards developing UQ (in particular Bayesian) methodologies for their efficient use in aerothermodynamics models and experimental data. Based on that knowledge, I would assume you have come to SoFIA because whatever is in here will suit you as an aerothermodynamicist without the need to bend over backwards trying to understand those obscure mathematicians in search for a proper library.

I have often found libraries that do uncertainty propagation or Bayesian inference to be very naive. I'll explain myself. They generally use data formats that are only apt to be used with python functions. They assume the model you want to do UQ on is defined on python, which is generally not the case for our community. Working around these issues often require more work than just coding the whole thing yourself. Yes, such libraries have lots of modules and functionalities but it is not so worth to pay the price of having to adapt your model (that very complex code, made by many people's contributions over many years) to suit those libraries needs. That's the philosophy out of which SoFIA is conceived.

SoFIA contains the methods I needed to use during my PhD, nothing more. The methods are implemented in a way that it is very open to suit different models' needs. In general, SoFIA offers a workflow where you can generate samples of input variables that need to be used by the code of your choice. Then you can come back to SoFIA with the evaluations of the model and use other functionalities like computing Sobol indices or fitting a Gaussian Process model. SoFIA does not handle your model for you or ask you to format the model in a specific way. This is great for obtaining full flexibility while enjoying the library's methods.

Finally, the purpose of SoFIA is to also grow. If you happen to need to implement a new method, you can do so and merge it to the rest of SoFIA, so that the library keeps growing.

\section{Structure}

In within the SoFIA library, different modules are defined. This section is structured according to the different modules that can be found in SoFIA.\\

Create mind map or something and linkages to external functions and codes.

\section{Usage}

In this section, we describe the basic imports and object calls to use the different features of the SoFIA library.\\

\noindent\textbf{\small Sensitivity Analysis (\texttt{SA}).} This module can be found in \texttt{SoFIA/SA}. In there, you can find the \texttt{Sobol.py} file with different utilities. We have the \texttt{sampling\_sequence} which generates the matrices with the inputs that need evaluations from our model, and \texttt{indices} which takes on the resulting evaluations and compute the first and total order Sobol' indices. You can print out the matrix with the inputs (each row contains all the inputs for one evaluation of the model) and then generate the evaluations the way you prefer. Then you come back and get your indices. The following code snippet shows an example of usage of this module. In \texttt{[...]} you can include the body of your code, namely, the function you want to perform sensitivity analysis on and any other relevant characteristics.\\
\quoteON
```
\texttt{import sofia.Sobol as sbl}\\

\texttt{[...] \# Definition of the function to be evaluated}\\

\texttt{SA = sbl.Sobol() \# Instantiation of sensitivity analysis object}\\

\texttt{samples = SA.sampling\_sequence(n\_samples,n\_variables,['dist vars'],None)}\\

\texttt{[...] \# Evaluation of function f on the samples computed}\\

\texttt{SA.indices(f,n\_samples,n\_variables)}

```
\quoteOFF

The concrete example with the Ishigami function can be found in \texttt{SA/example.py}.\\

\noindent\textbf{\small Probability distributions (\texttt{Probability\_distributions}).} This module can be found in \texttt{SoFIA/Probability\_distributions}. Several utilities are defined in \texttt{distributions.py}. There are two implementations: uniform distribution and Gaussian distribution. Both objects can be used with an arbitrary number of dimensions, although the resulting n-dimensional distributions do not account for correlation among variables. To instantiate a distribution object we prescribe the number of dimensions and the hyperparameters: lower and upper bounds for uniform distributions, and mean and standard deviation for Gaussian distributions. We can then compute PDF values, log-PDF values (useful for inverse problems), CDF values, inverse CDF values (to sample arbitrary distributions based on uniform or Gaussians), and sample from the distribution itself. A code snippet with a usage example is shown hereafter.\\

\quoteON
```
\texttt{import sofia.distributions as dist}\\

\texttt{hyp = [[a,b]] \# Hyperparameters for a 1D distribution}\\

\texttt{D = dist.Uniform(n\_dimensions,hyp) or dist.Gaussian(n\_dimensions,hyp) \# Instantiation of probability distribution object: either 1D uniform or Gaussian}\\

\texttt{x = np.linspace(-10.,10.,1000)}\\

\texttt{D.get\_cdf\_values(x) \# get CDF values of the distribution}\\

\texttt{D.get\_samples(n\_samples) \# Sample the distribution}

```
\quoteOFF

\noindent\textbf{\small Inverse uncertainty propagation (\texttt{Iprop}).} This module is placed in \texttt{SoFIA/Iprop}. Utilities can be found in the script \texttt{sampler.py} with which a Markov chain based on the Metropolis Hastings algorithm can be built. For the instantiation of the sampler we have to specify an initial covariance function \texttt{covinit}, a loglikelihood function \texttt{fun\_in}, and the number of samples to burn for the adaptation of the covariance matrix \texttt{nburn}. The chain is initiated with the parameters specified in the utility \texttt{seed}, while the \texttt{Burn} routine adapts the covariance matrix with the presribed number of draws \texttt{nburn}. The function \texttt{DoStep} advances the Markov chain and generates the useful samples to be used in the analysis of the posterior distribution. The script \texttt{sampler.py} also includes two diagnostic techniques: the plotting of the trace through the \texttt{chain\_visual} function, and the computation of the autocorrelation function with \texttt{autocorr}. For the chain diagnostics object, we must specify the chain samples and a dictionary that links the position of the variables in the vector of variables used for the chain construction to a variable name for plotting purposes. A code snippet with the typical usage of this module is shown hereafter.\\

\quoteON
```
\texttt{import sofia.sampler as mcmc}\\

\texttt{[...] \# Model to be calibrated and corresponding definition of the loglikelihood function of the experiments}

\texttt{sampler = mcmc.metropolis(np.identity(n\_dim)*0.01, log\_lik, n\_samples) \# Instantiation of the MCMC sampler}\\

\texttt{[...] \# Definition of the initial values of the parameters to be calibrated}\\

\texttt{sampler.seed(initial\_values\_of\_parameters)}\\

\texttt{sampler.Burn()}\\

\texttt{for i in range(n\_samples):}\\
\texttt{sampler.DoStep(1)}\\

\texttt{sampler\_diag = mcmc.diagnostics(chain,dict\_var) \# Instantiation of the chain diagnostics object}\\

\texttt{sampler\_diag.chain\_visual(n\_plots,var)}\\

\texttt{sampler\_diag.autocorr(n\_lags,n\_plots,var)}

```
\quoteOFF

\noindent\textbf{\small Forward uncertainty propagation (\texttt{Fprop}).} Under development. For the moment, it includes a simple polynomial chaos construction. Sample-based methods can be built with the functionalities already included in the library. \\

\quoteON
```
\texttt{import sofia.pc as pce}\\
\texttt{import sofia.distributions as dist}\\

\texttt{\# Generate Gaussian samples to use}\\

\texttt{N = n\_samples}\\

\texttt{\# Definition of the distribution used in the PCE construction}\\

\texttt{hyp=[[0.,1.]]}\\
\texttt{G = dist.Gaussian(1,hyp)}\\
\texttt{S = G.get\_samples(N)}\\

\texttt{\# Definition of the targetted distribution}\\

\texttt{hypU=[[0.,1.]]}\\
\texttt{target = dist.Uniform(1,hypU)}\\

\texttt{h = target.fun\_icdf() \# Definition of the inverse CDF function of the target distribution}\\

\texttt{ki\_uniform = pce.approximate\_rv\_coeffs(n\_polynomials, h)}\\
\texttt{k = pce.generate\_rv(ki\_uniform, S) \# With k we should recover the target distribution}

```
\quoteOFF

\noindent\textbf{\small Applications (\texttt{Applications}).} Under development. It will include scripts with which to reproduce the results obtained in my PhD thesis. These scripts will put together different modules from SoFIA to perform all the computations. The only inputs to be provided will be the external model evaluations and inputs from which to construct Gaussian process surrogates and carry out the inverse and/or forward analyses. The data from the simulations I used in my thesis will also be available in another repository.\\

\section{Examples}

Particular examples of the usage of the different modules depicted in the previous section can be found in \texttt{SoFIA/examples}. Here we describe some of them and show the results for verification purposes.\\

\noindent \textbf{\small Sensitivity Analysis (\texttt{SA}).} Here we work with the Ishigami function

\begin{equation}
    f(x) = \sin(x) + a \cdot \sin(y)^{2} + b \cdot z^{4}\sin(x),
\end{equation}
for which we compute sensitivity indices for $x, y, z$ with respect to the model output $f$. The model parameters $a$ and $b$ are set to the values 7 and 0.1, respectively.

Results and plots for the particular examples I have in SoFIA


\end{document}
